{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMU9KZMyTD5Ds4Op0/7/pM/",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/prismvale/Podcast-highlights/blob/main/31oct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title\n",
        "!pip install openai-whisper yt-dlp webvtt-py transformers torch gradio --quiet\n",
        "\n",
        "# ==============================\n",
        "# 🎥 AI-Powered Auto Clip Generator (Colab + Gradio Frontend)\n",
        "# ==============================\n",
        "import os\n",
        "import re\n",
        "import subprocess\n",
        "import yt_dlp\n",
        "import whisper\n",
        "import webvtt\n",
        "import gradio as gr\n",
        "from transformers import pipeline\n",
        "\n",
        "# ==============================\n",
        "# ⚙️ Core Logic Function\n",
        "# ==============================\n",
        "def process_youtube_video(url):\n",
        "    try:\n",
        "        # ---------- CLEANUP ----------\n",
        "        patterns = [\"video.mp4\", \"video.en.vtt\", \"clip_.*\\.mp4\", \"clip_.*\\.vtt\"]\n",
        "        for pattern in patterns:\n",
        "            for file in [f for f in os.listdir() if re.fullmatch(pattern.replace(\"*\", \".*\"), f)]:\n",
        "                try:\n",
        "                    os.remove(file)\n",
        "                except Exception:\n",
        "                    pass\n",
        "\n",
        "        video_file = \"video.mp4\"\n",
        "        sub_file = \"video.en.vtt\"\n",
        "\n",
        "        # ---------- DOWNLOAD ----------\n",
        "        ydl_opts = {\"format\": \"mp4\", \"outtmpl\": \"video.%(ext)s\"}\n",
        "        with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "            ydl.download([url])\n",
        "\n",
        "        # ---------- WHISPER ----------\n",
        "        model = whisper.load_model(\"tiny\")\n",
        "        result = model.transcribe(video_file)\n",
        "\n",
        "        with open(sub_file, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(\"WEBVTT\\n\\n\")\n",
        "            for i, seg in enumerate(result[\"segments\"]):\n",
        "                start, end, text = seg[\"start\"], seg[\"end\"], seg[\"text\"].strip()\n",
        "                def sec_to_vtt(s):\n",
        "                    h = int(s // 3600)\n",
        "                    m = int((s % 3600) // 60)\n",
        "                    sec = s % 60\n",
        "                    return f\"{h:02}:{m:02}:{sec:06.3f}\"\n",
        "                f.write(f\"{i+1}\\n{sec_to_vtt(start)} --> {sec_to_vtt(end)}\\n{text}\\n\\n\")\n",
        "\n",
        "        # ---------- GROUP CHUNKS ----------\n",
        "        grouped_chunks = []\n",
        "        current_chunk, chunk_start, chunk_end = [], None, None\n",
        "        max_chunk_duration, min_chunk_duration = 25.0, 10.0\n",
        "\n",
        "        for caption in webvtt.read(sub_file):\n",
        "            start_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(caption.start.split(\":\"))))\n",
        "            end_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(caption.end.split(\":\"))))\n",
        "\n",
        "            if chunk_start is None:\n",
        "                chunk_start = start_sec\n",
        "\n",
        "            sentences = [s.strip() for s in re.split(r'(?<=[.!?]) +', caption.text) if s.strip()]\n",
        "\n",
        "            for sentence in sentences:\n",
        "                current_chunk.append(sentence)\n",
        "                chunk_end = end_sec\n",
        "                if chunk_start is not None and chunk_end is not None and (chunk_end - chunk_start >= max_chunk_duration):\n",
        "                    if grouped_chunks and (chunk_end - chunk_start < min_chunk_duration):\n",
        "                        grouped_chunks[-1][\"end\"] = chunk_end\n",
        "                        grouped_chunks[-1][\"text\"] += \" \" + \" \".join(current_chunk)\n",
        "                    else:\n",
        "                        grouped_chunks.append({\n",
        "                            \"start\": chunk_start,\n",
        "                            \"end\": chunk_end,\n",
        "                            \"text\": \" \".join(current_chunk)\n",
        "                        })\n",
        "                    current_chunk, chunk_start, chunk_end = [], None, None\n",
        "\n",
        "        if current_chunk:\n",
        "            if chunk_start is not None and chunk_end is not None:\n",
        "                if grouped_chunks and (chunk_end - chunk_start < min_chunk_duration):\n",
        "                    grouped_chunks[-1][\"end\"] = chunk_end\n",
        "                    grouped_chunks[-1][\"text\"] += \" \" + \" \".join(current_chunk)\n",
        "                else:\n",
        "                    grouped_chunks.append({\n",
        "                        \"start\": chunk_start,\n",
        "                        \"end\": chunk_end,\n",
        "                        \"text\": \" \".join(current_chunk)\n",
        "                    })\n",
        "\n",
        "        # ---------- ANALYSIS ----------\n",
        "        sentiment_model = pipeline(\"sentiment-analysis\", model=\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
        "        emotion_model = pipeline(\"text-classification\",\n",
        "                                 model=\"joeddav/distilbert-base-uncased-go-emotions-student\",\n",
        "                                 return_all_scores=True)\n",
        "\n",
        "        scored_chunks = []\n",
        "        for chunk in grouped_chunks:\n",
        "            text = chunk[\"text\"]\n",
        "            sentiment = sentiment_model(text[:512])[0]\n",
        "            emotions = emotion_model(text[:512])[0]\n",
        "            top_emotion = max(emotions, key=lambda x: x['score'])\n",
        "            score = sentiment['score'] * 5\n",
        "            if top_emotion['label'] in [\"joy\", \"excitement\", \"anger\", \"sadness\"]:\n",
        "                score += top_emotion['score'] * 5\n",
        "            scored_chunks.append({\n",
        "                \"start\": chunk[\"start\"],\n",
        "                \"end\": chunk[\"end\"],\n",
        "                \"text\": text,\n",
        "                \"sentiment\": sentiment['label'],\n",
        "                \"emotion\": top_emotion['label'],\n",
        "                \"score\": score\n",
        "            })\n",
        "\n",
        "        # ---------- SELECT TOP 3 ----------\n",
        "        scored_chunks = sorted(scored_chunks, key=lambda x: x[\"score\"], reverse=True)\n",
        "        top3 = scored_chunks[:3]\n",
        "\n",
        "        result_text = \"🔥 **Top 3 Engaging Moments:**\\n\\n\"\n",
        "        #for i, c in enumerate(top3, 1):\n",
        "            #result_text += f\"### 🎬 Clip {i}\\n\"\n",
        "            #result_text += f\"- Time: {c['start']:.2f}s → {c['end']:.2f}s\\n\"\n",
        "            #result_text += f\"- Emotion: {c['emotion']} | Sentiment: {c['sentiment']} | Score: {c['score']:.2f}\\n\"\n",
        "            #result_text += f\"- Text: {c['text'][:150]}...\\n\\n\"\n",
        "\n",
        "        # ---------- EXTRACT CLIPS ----------\n",
        "        captions = list(webvtt.read(sub_file))\n",
        "        def format_time(seconds):\n",
        "            h = int(seconds // 3600)\n",
        "            m = int((seconds % 3600) // 60)\n",
        "            s = seconds % 60\n",
        "            return f\"{h:02d}:{m:02d}:{s:06.3f}\".replace('.', ',')\n",
        "\n",
        "        video_files = []\n",
        "        for i, clip in enumerate(top3, 1):\n",
        "            start, end = clip[\"start\"], clip[\"end\"]\n",
        "            out_video, out_sub = f\"clip_{i}.mp4\", f\"clip_{i}.vtt\"\n",
        "            cmd = [\"ffmpeg\", \"-y\", \"-i\", video_file, \"-ss\", str(start), \"-to\", str(end), \"-c\", \"copy\", out_video]\n",
        "            subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
        "            video_files.append(out_video)\n",
        "\n",
        "            selected = []\n",
        "            for c in captions:\n",
        "                start_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(c.start.split(\":\"))))\n",
        "                end_sec = sum(float(x) * 60 ** i for i, x in enumerate(reversed(c.end.split(\":\"))))\n",
        "                if start_sec >= start and end_sec <= end:\n",
        "                    new_start, new_end = start_sec - start, end_sec - start\n",
        "                    selected.append(f\"{format_time(new_start)} --> {format_time(new_end)}\\n{c.text}\\n\")\n",
        "\n",
        "            with open(out_sub, \"w\", encoding=\"utf-8\") as f:\n",
        "                f.write(\"WEBVTT\\n\\n\" + \"\\n\".join(selected))\n",
        "\n",
        "        return result_text, video_files[0] if len(video_files) > 0 else None, video_files[1] if len(video_files) > 1 else None, video_files[2] if len(video_files) > 2 else None\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Error: {e}\", None, None, None\n",
        "\n",
        "# ==============================\n",
        "# 🎨 Gradio UI\n",
        "# ==============================\n",
        "demo = gr.Interface(\n",
        "    fn=process_youtube_video,\n",
        "    inputs=gr.Textbox(label=\"Enter YouTube URL\"),\n",
        "    outputs=[\n",
        "        gr.Markdown(label=\"Results\"),\n",
        "        gr.Video(label=\"Clip 1\"),\n",
        "        gr.Video(label=\"Clip 2\"),\n",
        "        gr.Video(label=\"Clip 3\")\n",
        "    ],\n",
        "    title=\"🎥 AI Auto Clip Generator\",\n",
        "    description=(\n",
        "    \"Automatically extracts top 3 emotional or engaging clips from a YouTube video \"\n",
        "    \"using Whisper + NLP sentiment/emotion analysis.\\n\\n\"\n",
        "    \"⏱️ For about a 1-hour video, processing takes around 4–5 minutes (first run may be slower).\"),\n",
        "    allow_flagging=\"never\"\n",
        ")\n",
        "\n",
        "demo.launch(debug=True)\n"
      ],
      "metadata": {
        "id": "HLEkMM-Q4bk_"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}